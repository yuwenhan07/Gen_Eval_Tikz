{
  "timestamp": "2025-09-08T13:23:13",
  "model": "ernie-4.5-turbo-vl-preview",
  "temperature": 0.8,
  "top_p": 0.9,
  "max_tokens": 2048,
  "sample_index": 69,
  "image_rel_path": "images/test_69.png",
  "caption": "\\textbf{Accuracy comparison of Continued Pretrained models on English (Left, Right) and Turkish (Right) question answering tasks and demonstrating the original language catastrophic forgetting while learning the new language.} In the table on the left, the performance of our Hamza$_{\\scriptsize Mistral}$ and Hamza$_{\\scriptsize GPT2-xl}$ models that are adapted on Turkish together with the original Mistral 7B and GPT2-xl. We present the result of our ablation study, where the performance of the adapted models is given by progressively enlarging the pretraining corpus size from 0.1 GB to 5 GB. Here, the zero and few-show accuracies were evaluated on the original ARC and TruthfulQA. The figure on the right illustrates the Mistral model's results on both Turkish and English versions of the ARC dataset, highlighting its improved performance in Turkish and decreasing performance in English with continued pretraining.",
  "snippet_input": "",
  "raw_output": "Here is the LaTeX code to generate the given image using TikZ and PGFPlots:\n\n```latex\n\\documentclass{standalone}\n\\usepackage{pgfplots}\n\\usepackage{pgfplotstable}\n\\pgfplotsset{compat=1.17}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\begin{axis}[\n    title={Comparison of ARC Scores of Mistral Model},\n    xlabel={\\# of Turkish Tokens},\n    ylabel={English \\& Turkish ARC Acc.},\n    grid=major,\n    legend style={cells={align=left}},\n    legend pos=north east,\n]\n\n\\addplot[color=blue, mark=o] coordinates {\n    (0, 33)\n    (50000000, 34)\n    (130000000, 37)\n    (250000000, 36)\n    (500000000, 39)\n    (1100000000, 39)\n    (2500000000, 40)\n};\n\\addlegendentry{ARC-tr Acc.}\n\n\\addplot[color=red, mark=square] coordinates {\n    (0, 62)\n    (50000000, 57)\n    (130000000, 53)\n    (250000000, 52)\n    (500000000, 51)\n    (1100000000, 49)\n    (2500000000, 46)\n};\n\\addlegendentry{ARC-eng Acc.}\n\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}\n```\n\n### Explanation:\n\n- **Title**: Added as `Comparison of ARC Scores of Mistral Model`.\n- **X-axis**: Labeled as `# of Turkish Tokens`.\n- **Y-axis**: Labeled as `English & Turkish ARC Acc.`.\n- **Grid**: Added major grid lines for better readability.\n- **Legend**: Positioned at the north east, with left-aligned text for clarity.\n- **Plots**:\n  - **Blue Plot with Circles**: Represents `ARC-tr Acc.`.\n  - **Red Plot with Squares**: Represents `ARC-eng Acc.`.\n- **Coordinates**: Used the approximate values as inferred from the image.\n\nThis code will generate a plot that closely matches the visual appearance of the provided image.",
  "extracted_code": "\\documentclass{standalone}\n\\usepackage{pgfplots}\n\\usepackage{pgfplotstable}\n\\pgfplotsset{compat=1.17}\n\n\\begin{document}\n\n\\begin{tikzpicture}\n\\begin{axis}[\n    title={Comparison of ARC Scores of Mistral Model},\n    xlabel={\\# of Turkish Tokens},\n    ylabel={English \\& Turkish ARC Acc.},\n    grid=major,\n    legend style={cells={align=left}},\n    legend pos=north east,\n]\n\n\\addplot[color=blue, mark=o] coordinates {\n    (0, 33)\n    (50000000, 34)\n    (130000000, 37)\n    (250000000, 36)\n    (500000000, 39)\n    (1100000000, 39)\n    (2500000000, 40)\n};\n\\addlegendentry{ARC-tr Acc.}\n\n\\addplot[color=red, mark=square] coordinates {\n    (0, 62)\n    (50000000, 57)\n    (130000000, 53)\n    (250000000, 52)\n    (500000000, 51)\n    (1100000000, 49)\n    (2500000000, 46)\n};\n\\addlegendentry{ARC-eng Acc.}\n\n\\end{axis}\n\\end{tikzpicture}\n\n\\end{document}"
}